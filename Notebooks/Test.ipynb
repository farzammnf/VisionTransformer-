{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T13:48:55.889549Z","iopub.status.busy":"2024-06-14T13:48:55.889173Z","iopub.status.idle":"2024-06-14T13:49:00.406102Z","shell.execute_reply":"2024-06-14T13:49:00.404556Z","shell.execute_reply.started":"2024-06-14T13:48:55.889515Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import torchvision.models as models\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T13:49:07.159889Z","iopub.status.busy":"2024-06-14T13:49:07.159168Z","iopub.status.idle":"2024-06-14T13:49:07.164288Z","shell.execute_reply":"2024-06-14T13:49:07.163239Z","shell.execute_reply.started":"2024-06-14T13:49:07.159858Z"},"trusted":true},"outputs":[],"source":["# Best hyperparameters\n","patch_size = 8\n","embedding_dim = 256\n","num_heads = 4\n","num_layers = 6\n","learning_rate = 5e-5\n","batch_size = 16\n","num_epochs = 10  \n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T13:49:09.861075Z","iopub.status.busy":"2024-06-14T13:49:09.860433Z","iopub.status.idle":"2024-06-14T13:49:15.316279Z","shell.execute_reply":"2024-06-14T13:49:15.315467Z","shell.execute_reply.started":"2024-06-14T13:49:09.861045Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 84830153.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["# CIFAR-10 dataset\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T13:49:17.298009Z","iopub.status.busy":"2024-06-14T13:49:17.297649Z","iopub.status.idle":"2024-06-14T13:49:17.608579Z","shell.execute_reply":"2024-06-14T13:49:17.607663Z","shell.execute_reply.started":"2024-06-14T13:49:17.297978Z"},"trusted":true},"outputs":[],"source":["class VisionTransformer(nn.Module):\n","    def __init__(self, num_classes, patch_size, embedding_dim, num_heads, num_layers):\n","        super(VisionTransformer, self).__init__()\n","        self.patch_embedding = nn.Conv2d(3, embedding_dim, kernel_size=patch_size, stride=patch_size)\n","        self.positional_encoding = nn.Parameter(torch.randn(1, (128 // patch_size) ** 2 + 1, embedding_dim))\n","        self.transformer_layers = nn.ModuleList([\n","            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads) for _ in range(num_layers)\n","        ])\n","        self.fc = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        x = self.patch_embedding(x)\n","        x = x.flatten(2).transpose(1, 2)\n","        x = torch.cat((x, self.positional_encoding[:, :x.size(1), :].repeat(batch_size, 1, 1)), dim=1)\n","        for layer in self.transformer_layers:\n","            x = layer(x)\n","        x = x.mean(dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","# Initialize the model\n","model = VisionTransformer(num_classes=10, patch_size=patch_size, embedding_dim=embedding_dim, \n","                          num_heads=num_heads, num_layers=num_layers).to(device)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T13:49:21.250781Z","iopub.status.busy":"2024-06-14T13:49:21.250415Z","iopub.status.idle":"2024-06-14T13:49:21.256042Z","shell.execute_reply":"2024-06-14T13:49:21.255102Z","shell.execute_reply.started":"2024-06-14T13:49:21.250752Z"},"trusted":true},"outputs":[],"source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T13:49:21.679050Z","iopub.status.busy":"2024-06-14T13:49:21.678439Z","iopub.status.idle":"2024-06-14T15:19:18.545521Z","shell.execute_reply":"2024-06-14T15:19:18.544569Z","shell.execute_reply.started":"2024-06-14T13:49:21.679021Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/10: 100%|██████████| 3125/3125 [08:26<00:00,  6.17it/s, Loss=1.63, Accuracy=30.3]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Validation Accuracy: 37.45%\n","Saving model with validation accuracy: 37.45%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/10: 100%|██████████| 3125/3125 [08:27<00:00,  6.15it/s, Loss=1.57, Accuracy=41.3]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/10], Validation Accuracy: 44.08%\n","Saving model with validation accuracy: 44.08%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/10: 100%|██████████| 3125/3125 [08:27<00:00,  6.16it/s, Loss=1.42, Accuracy=47.8] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/10], Validation Accuracy: 50.34%\n","Saving model with validation accuracy: 50.34%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/10: 100%|██████████| 3125/3125 [08:26<00:00,  6.16it/s, Loss=1.93, Accuracy=51.8] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/10], Validation Accuracy: 53.31%\n","Saving model with validation accuracy: 53.31%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/10: 100%|██████████| 3125/3125 [08:27<00:00,  6.16it/s, Loss=1.22, Accuracy=54.9] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/10], Validation Accuracy: 55.96%\n","Saving model with validation accuracy: 55.96%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6/10: 100%|██████████| 3125/3125 [08:27<00:00,  6.16it/s, Loss=1.55, Accuracy=57.2] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/10], Validation Accuracy: 58.42%\n","Saving model with validation accuracy: 58.42%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7/10: 100%|██████████| 3125/3125 [08:27<00:00,  6.16it/s, Loss=1.44, Accuracy=59.2] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/10], Validation Accuracy: 60.60%\n","Saving model with validation accuracy: 60.60%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8/10: 100%|██████████| 3125/3125 [08:27<00:00,  6.16it/s, Loss=0.762, Accuracy=60.9]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [8/10], Validation Accuracy: 62.63%\n","Saving model with validation accuracy: 62.63%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9/10: 100%|██████████| 3125/3125 [08:26<00:00,  6.17it/s, Loss=1.25, Accuracy=62.6] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch [9/10], Validation Accuracy: 62.99%\n","Saving model with validation accuracy: 62.99%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10/10: 100%|██████████| 3125/3125 [08:26<00:00,  6.16it/s, Loss=0.902, Accuracy=63.6]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [10/10], Validation Accuracy: 63.86%\n","Saving model with validation accuracy: 63.86%\n"]}],"source":["# Training loop\n","best_accuracy = 0.0\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0.0\n","    correct = 0\n","    total_samples = 0\n","    \n","    # Use tqdm for progress bar\n","    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}')\n","    \n","    for i, (images, labels) in progress_bar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track training accuracy\n","        _, predicted = torch.max(outputs, 1)\n","        correct += (predicted == labels).sum().item()\n","        total_samples += labels.size(0)\n","\n","        # Track total loss\n","        total_loss += loss.item()\n","\n","        # Update progress bar\n","        progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': (correct / total_samples) * 100})\n","\n","    # Evaluate on the validation set\n","    model.eval()\n","    correct = 0\n","    total_samples = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            correct += (predicted == labels).sum().item()\n","            total_samples += labels.size(0)\n","\n","    # Calculate accuracy\n","    accuracy = correct / total_samples * 100\n","\n","    # Print validation accuracy for each epoch\n","    print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%')\n","\n","    # Save the model if it has the best accuracy\n","    if accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        # Save model to /kaggle/working/\n","        torch.save(model.state_dict(), '/kaggle/working/best_model.pth')\n","        print(f'Saving model with validation accuracy: {best_accuracy:.2f}%')\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T15:19:18.547732Z","iopub.status.busy":"2024-06-14T15:19:18.547417Z","iopub.status.idle":"2024-06-14T15:19:51.006088Z","shell.execute_reply":"2024-06-14T15:19:51.005198Z","shell.execute_reply.started":"2024-06-14T15:19:18.547707Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Vision Transformer Test Accuracy: 63.86%\n"]}],"source":["# Load the best model\n","model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n","\n","# Evaluate on the test set\n","model.eval()\n","correct = 0\n","total_samples = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        correct += (predicted == labels).sum().item()\n","        total_samples += labels.size(0)\n","\n","# Calculate accuracy\n","vit_accuracy = correct / total_samples * 100\n","print(f'Vision Transformer Test Accuracy: {vit_accuracy:.2f}%')\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T15:29:24.253849Z","iopub.status.busy":"2024-06-14T15:29:24.253004Z","iopub.status.idle":"2024-06-14T15:29:25.905429Z","shell.execute_reply":"2024-06-14T15:29:25.904681Z","shell.execute_reply.started":"2024-06-14T15:29:24.253815Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["import torch\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","\n","# ImageNet normalization parameters\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# CIFAR-10 dataset\n","train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T15:29:36.538861Z","iopub.status.busy":"2024-06-14T15:29:36.538172Z","iopub.status.idle":"2024-06-14T16:52:04.509572Z","shell.execute_reply":"2024-06-14T16:52:04.508551Z","shell.execute_reply.started":"2024-06-14T15:29:36.538830Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/5: 100%|██████████| 3125/3125 [02:41<00:00, 19.31it/s]\n","Epoch 2/5: 100%|██████████| 3125/3125 [02:41<00:00, 19.40it/s]\n","Epoch 3/5: 100%|██████████| 3125/3125 [02:41<00:00, 19.40it/s]\n","Epoch 4/5: 100%|██████████| 3125/3125 [02:40<00:00, 19.41it/s]\n","Epoch 5/5: 100%|██████████| 3125/3125 [02:40<00:00, 19.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Fine-Tuned ResNet-18 Test Accuracy: 94.15%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5: 100%|██████████| 3125/3125 [13:35<00:00,  3.83it/s]\n","Epoch 2/5: 100%|██████████| 3125/3125 [13:33<00:00,  3.84it/s]\n","Epoch 3/5: 100%|██████████| 3125/3125 [13:32<00:00,  3.85it/s]\n","Epoch 4/5: 100%|██████████| 3125/3125 [13:31<00:00,  3.85it/s]\n","Epoch 5/5: 100%|██████████| 3125/3125 [13:30<00:00,  3.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Fine-Tuned VGG-16 Test Accuracy: 88.98%\n"]}],"source":["import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.models as models\n","from tqdm import tqdm\n","\n","\n","def fine_tune_and_evaluate(model, train_loader, test_loader, num_epochs=5):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","    \n","    # Evaluation\n","    model.eval()\n","    correct = 0\n","    total_samples = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            correct += (predicted == labels).sum().item()\n","            total_samples += labels.size(0)\n","    accuracy = correct / total_samples * 100\n","    return accuracy\n","\n","# Load pre-trained ResNet-18 and VGG-16, replace final layer, and fine-tune\n","resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","resnet18.fc = nn.Linear(resnet18.fc.in_features, 10)\n","resnet18_accuracy = fine_tune_and_evaluate(resnet18, train_loader, test_loader)\n","print(f'Fine-Tuned ResNet-18 Test Accuracy: {resnet18_accuracy:.2f}%')\n","\n","vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n","vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 10)\n","vgg16_accuracy = fine_tune_and_evaluate(vgg16, train_loader, test_loader)\n","print(f'Fine-Tuned VGG-16 Test Accuracy: {vgg16_accuracy:.2f}%')\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T16:52:04.512493Z","iopub.status.busy":"2024-06-14T16:52:04.511838Z","iopub.status.idle":"2024-06-14T16:52:04.517575Z","shell.execute_reply":"2024-06-14T16:52:04.516668Z","shell.execute_reply.started":"2024-06-14T16:52:04.512457Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Vision Transformer Test Accuracy: 63.86%\n","ResNet-18 Test Accuracy: 94.15%\n","VGG-16 Test Accuracy: 88.98%\n"]}],"source":["print(f'Vision Transformer Test Accuracy: {vit_accuracy:.2f}%')\n","print(f'ResNet-18 Test Accuracy: {resnet18_accuracy:.2f}%')\n","print(f'VGG-16 Test Accuracy: {vgg16_accuracy:.2f}%')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
